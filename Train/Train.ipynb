{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "from keras import optimizers, losses, metrics, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, load_model\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = r\"C:\\Rotated Equal Data\"\n",
    "inputPath = r\"D:\\Programing\\Projects\\ESP32-TFLite-Knock-Detector\\Data Collection\\Data\\Equal Data\"\n",
    "\n",
    "commands = np.array(tf.io.gfile.listdir(str(inputPath)))\n",
    "print(commands)\n",
    "N_CLASSES = len(commands)\n",
    "print(N_CLASSES)\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "filesArray = []\n",
    "for path, subdirs, files in os.walk(inputPath):\n",
    "    for name in files:\n",
    "        filesArray.append(os.path.join(path + \"\\\\\", name))\n",
    "print(len(filesArray))\n",
    "\n",
    "\n",
    "\n",
    "#for i in filesArray:\n",
    "    #print(i)\n",
    "    \n",
    "max_length = 1000\n",
    "df = []\n",
    "labels = []\n",
    "for filename in filesArray:\n",
    "    data = np.loadtxt(filename, dtype=np.float32)\n",
    "    \n",
    "    data = data/2 #Normalise\n",
    "    data = abs(data)\n",
    "\n",
    "    if len(data) < max_length:\n",
    "        # pad the time series with zeros to a length of 1000\n",
    "        data = np.pad(data, [(0, max_length - len(data)), (0, 0)], mode='constant')\n",
    "    elif len(data) > max_length:\n",
    "        # truncate the time series to a length of 1000\n",
    "        data = data[:max_length, :]\n",
    "    a = filename.split(\"\\\\\")[-2]\n",
    "    if a == \"True\":\n",
    "        label = [1,0]\n",
    "    else:\n",
    "        label = [0,1]\n",
    "    labels.append(label)\n",
    "\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "    df.append(data)\n",
    "\n",
    "df_array = np.array(df)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "df_array, labels_array = unison_shuffled_copies(df_array, labels_array)\n",
    "\n",
    "df_array, labels_array = unison_shuffled_copies(df_array, labels_array)\n",
    "\n",
    "print(df_array.shape)\n",
    "print(labels_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the log directory for TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        print(0.001 * tf.math.exp(-0.02*((epoch-5))))\n",
    "        return 0.001 * tf.math.exp(-0.02*((epoch-5)))\n",
    "\n",
    "# Define the TensorBoard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    \n",
    "total_epochs = 0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(L.Input(shape=(1000,1, 1)))\n",
    "\n",
    "# Block 1\n",
    "model.add(L.Conv2D(8, (100, 1), activation='relu'))\n",
    "model.add(L.AveragePooling2D((2, 1), strides=(2, 1)))\n",
    "\n",
    "# Block 2\n",
    "model.add(L.Conv2D(16, (15, 1), activation='relu'))\n",
    "model.add(L.AveragePooling2D((2, 1)))\n",
    "\n",
    "# Block 3\n",
    "model.add(L.Conv2D(16, (5, 1), activation='relu'))\n",
    "model.add(L.AveragePooling2D((2, 1)))\n",
    "\n",
    "# Block 4\n",
    "model.add(L.Conv2D(16, (3, 1), activation='relu'))\n",
    "model.add(L.AveragePooling2D((2, 1)))\n",
    "\n",
    "# Fully connected L\n",
    "model.add(L.Flatten())\n",
    "model.add(L.Dense(32, activation='relu'))\n",
    "model.add(L.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.SGD(learning_rate = 0.001, momentum=0.9),\n",
    "              loss=losses.CategoricalCrossentropy(),\n",
    "              metrics=[metrics.CategoricalAccuracy()])\n",
    "\n",
    "# model.build((235, 4001, 1))\n",
    "print(model.summary())\n",
    "\n",
    "new_epochs = 3\n",
    "model.fit(df_array, labels_array, shuffle=1,  initial_epoch=total_epochs, epochs=total_epochs+new_epochs, validation_split=0.25, batch_size = 2, callbacks=[tensorboard_callback,lr_scheduler])\n",
    "total_epochs += new_epochs\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate = 0.001),\n",
    "              loss=losses.CategoricalCrossentropy(),\n",
    "              metrics=[metrics.CategoricalAccuracy()])\n",
    "\n",
    "new_epochs = 8\n",
    "model.fit(df_array, labels_array, shuffle=1,  initial_epoch=total_epochs, epochs=total_epochs+new_epochs, validation_split=0.05, batch_size = 32, callbacks=[tensorboard_callback,lr_scheduler])\n",
    "total_epochs += new_epochs\n",
    "\n",
    "model.evaluate(df_array, labels_array, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(df_array, labels_array, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Keras model\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Define the pruning parameters\n",
    "pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "    initial_sparsity=0.1,\n",
    "    final_sparsity=0.99,\n",
    "    begin_step=0,\n",
    "    end_step=19\n",
    ")\n",
    "\n",
    "# Define the pruning callback\n",
    "pruning_callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir='/tmp')\n",
    "]\n",
    "\n",
    "# Create a pruned model\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
    "\n",
    "# Train the pruned model\n",
    "pruned_model.compile(optimizer=tf.optimizers.Adam(learning_rate= 0.000001),\n",
    "              loss=losses.CategoricalCrossentropy(),\n",
    "              metrics=[metrics.CategoricalAccuracy()])\n",
    "pruned_model.fit(df_array, labels_array, shuffle=1, epochs=20, validation_split=0.25, batch_size=128, callbacks=pruning_callbacks)\n",
    "\n",
    "# Remove the pruning wrappers from the model\n",
    "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model.save('pruned_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Keras model\n",
    "pruned_model = tf.keras.models.load_model('pruned_model.h5')\n",
    "pruned_model.compile(optimizer=tf.optimizers.Adam(learning_rate= 0.0001),\n",
    "              loss=losses.CategoricalCrossentropy(),\n",
    "              metrics=[metrics.CategoricalAccuracy()])\n",
    "#pruned_model.evaluate(df_array, labels_array, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### NON QUANTIZED ############\n",
    "pruned_model = tf.keras.models.load_model(\"pruned_model.h5\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### QUANTIZED ############\n",
    "pruned_model = tf.keras.models.load_model(\"pruned_model.h5\")\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in df_array:\n",
    "        input_value = np.expand_dims(input_value, axis=0)\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT, tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "#converter.target_spec.supported_ops = [tf.float16]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "input_tensor_index = input_details[0]['index']\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "# Get the scale and zero-point values from the input tensor\n",
    "input_tensor_scale = input_details[0]['quantization_parameters']['scales']\n",
    "input_tensor_zero_point = input_details[0]['quantization_parameters']['zero_points']\n",
    "\n",
    "output_tensor_scale = output_details[0]['quantization'][0]\n",
    "output_tensor_zero_point = output_details[0]['quantization'][1]\n",
    "\n",
    "print(\"Input tensor scale:\", input_tensor_scale)\n",
    "print(\"Input tensor zero-point:\", input_tensor_zero_point)\n",
    "\n",
    "print(\"Output tensor scale:\", output_tensor_scale)\n",
    "print(\"Output tensor zero-point:\", output_tensor_zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## For Quantized only ###################\n",
    "for j in range(len(df_array)):\n",
    "    for i in range(len(df_array[0])):\n",
    "        df_array[j][i][0] = int(df_array[j][i][0] / input_tensor_scale) + input_tensor_zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "for i in range(len(df_array)):\n",
    "    interpreter.set_tensor(input_details[0]['index'], df_array[i:i+1].astype(np.uint8))\n",
    "    interpreter.invoke()\n",
    "    output = (interpreter.get_tensor(output_details[0]['index']) - output_tensor_zero_point)*output_tensor_scale\n",
    "    #origional = model.predict(df_array[i:i+1], verbose=0)\n",
    "        \n",
    "    print(f'Label: {labels_array[i]}  tflite model prediction: {output}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
